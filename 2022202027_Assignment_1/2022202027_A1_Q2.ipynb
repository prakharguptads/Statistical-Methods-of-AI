{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "63ab7c40-c0da-4b75-a7b6-3822523f7218",
      "metadata": {
        "id": "63ab7c40-c0da-4b75-a7b6-3822523f7218"
      },
      "source": [
        "# Assignment 1\n",
        "\n",
        "## Question `2` (Decision Trees)\n",
        "\n",
        "| | |\n",
        "|-|-|\n",
        "| Course | Statistical Methods in AI |\n",
        "| Release Date | `19.01.2023` |\n",
        "| Due Date | `29.01.2023` |\n",
        "\n",
        "This assignment will have you working and experimenting with decision trees. Initially, you will be required to implement a decision tree classifier by choosing thresholds based on various impurity measures and reporting the scores. Later, you can experiment with the `scikit-learn` implementation of decision trees, and how various other parameters can be leveraged for better performance.\n",
        "\n",
        "The dataset is a very simple one, the [banknote authentication dataset](https://archive.ics.uci.edu/ml/datasets/banknote+authentication). It has 5 columns, the first 4 are the features, and the last one is the class label. The features are the variance, skewness, curtosis and entropy of the [wavelet transformed](https://en.wikipedia.org/wiki/Wavelet_transform) image of the banknote. The class label is 1 if the banknote is authentic, and 0 if it is forged. The data is present in `bankAuth.txt`. There are a total of 1372 samples in the dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2422c78-b240-48ca-b6a0-cc8b88562670",
      "metadata": {
        "id": "a2422c78-b240-48ca-b6a0-cc8b88562670"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "01616b64-1baa-4cda-9894-5ffac51ec662",
      "metadata": {
        "id": "01616b64-1baa-4cda-9894-5ffac51ec662"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "# from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn import *\n",
        "\n",
        "# additional imports if necessary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5653b9d8-2250-4b8e-820f-71ba9ccd5cf9",
      "metadata": {
        "id": "5653b9d8-2250-4b8e-820f-71ba9ccd5cf9"
      },
      "source": [
        "### Impurity Measures\n",
        "\n",
        "Decision trees are only as good as the impurity measure used to choose the best split. In this section, you will be required to implement the following impurity measures and use them to build a decision tree classifier.\n",
        "\n",
        "1. Gini Index\n",
        "2. Entropy\n",
        "3. Misclassification Error\n",
        "4. Log Loss\n",
        "\n",
        "Write functions that calculate the impurity measures for a given set of labels. The functions should take in a list of labels and return the impurity measure."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _gini(y):\n",
        "  w = np.array(y)\n",
        "  uniqw, inverse = np.unique(w, return_inverse=True)\n",
        "  hist = np.bincount(inverse)\n",
        "  ps = hist / len(y)\n",
        "  return 1-np.sum([p ** 2 for p in ps if p>0])\n",
        "      \n",
        "# Function to perform training with entropy.\n",
        "def _entropy(y):\n",
        "  w = np.array(y)\n",
        "  uniqw, inverse = np.unique(w, return_inverse=True)\n",
        "  hist = np.bincount(inverse)\n",
        "  ps = hist / len(y)\n",
        "  return -np.sum([p * np.log(p) for p in ps if p>0])\n",
        "\n",
        "def _mce(y):\n",
        "  w = np.array(y)\n",
        "  uniqw, inverse = np.unique(w, return_inverse=True)\n",
        "  hist = np.bincount(inverse)\n",
        "  ps = hist / len(y)\n",
        "  return 1-np.max([(p , 1-p) for p in ps if p>0])\n"
      ],
      "metadata": {
        "id": "k5qzKaLgpEM5"
      },
      "id": "k5qzKaLgpEM5",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6a37b889-d182-441a-b7ae-0803e65aff51",
      "metadata": {
        "id": "6a37b889-d182-441a-b7ae-0803e65aff51"
      },
      "outputs": [],
      "source": [
        "# your code here]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6bfcc68-388e-4da8-ad87-270ef1212cdc",
      "metadata": {
        "id": "a6bfcc68-388e-4da8-ad87-270ef1212cdc"
      },
      "source": [
        "### Decision Tree\n",
        "\n",
        "Fit a decision tree using any one of the above impurity measures with a depth of 3. This means you will have eight leaf nodes and seven internal nodes. Report the threshold values at each internal node and the impurity measure at the final leaf node with the label. Also report the accuracy of the classifier on the training and test data (instructions for splitting the data will be given in the end)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Node:\n",
        "    def __init__(self, feature=None, threshold=None, left=None, right=None,*,value=None):\n",
        "        self.feature = feature\n",
        "        self.threshold = threshold\n",
        "        self.left = left\n",
        "        self.value = value\n",
        "        self.right = right\n",
        "        \n",
        "    def is_leaf_node(self):\n",
        "        return self.value is not None"
      ],
      "metadata": {
        "id": "P6k8JaMl_-x4"
      },
      "id": "P6k8JaMl_-x4",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3939308e-294f-4d0e-a1af-1589906d5129",
      "metadata": {
        "id": "3939308e-294f-4d0e-a1af-1589906d5129"
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "class DecisionTree:\n",
        "    def __init__(self, min_samples_split=2, max_depth=100, n_features=None):\n",
        "        self.min_samples_split=min_samples_split\n",
        "        self.max_depth=max_depth\n",
        "        self.depth = None\n",
        "        self.n_features=n_features\n",
        "        self.root=None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.n_features = X.shape[1] if not self.n_features else min(X.shape[1],self.n_features)\n",
        "        print(\"Fitting the Data : \")\n",
        "        self.root = self.grow_tree(X, y)\n",
        "\n",
        "    def grow_tree(self, X, y, depth=0):\n",
        "        nsamples, n_feats = X.shape\n",
        "        yy = np.unique(y)\n",
        "        n_labels = len(yy)\n",
        "\n",
        "        # check the stopping criteria\n",
        "        if (depth>=self.max_depth or n_labels==1 or nsamples<self.min_samples_split):\n",
        "            leaf_value = Counter(y).most_common(1)[0][0]\n",
        "            # print(\"ggg\",depth,self.max_depth,n_labels,n_samples,self.min_samples_split)\n",
        "            return Node(value=leaf_value)\n",
        "\n",
        "        feat_idxs = np.random.choice(n_feats, self.n_features, replace=False)\n",
        "\n",
        "        # find the best split\n",
        "        best_feature, best_thresh = self.splitUtil(X, y, feat_idxs,depth)\n",
        "        print(\"Best feature : \",best_feature)\n",
        "        print(\"Best Threshold\",best_thresh)\n",
        "        # create child nodes\n",
        "        left_idxs = np.argwhere(X[:, best_feature] <= best_thresh).flatten()\n",
        "        right_idxs = np.argwhere(X[:, best_feature] > best_thresh).flatten()\n",
        "        left = self.grow_tree(X[left_idxs, :], y[left_idxs], depth+1)\n",
        "        right = self.grow_tree(X[right_idxs, :], y[right_idxs], depth+1)\n",
        "        return Node(best_feature, best_thresh, left, right)\n",
        "\n",
        "\n",
        "    def splitUtil(self, X, y, feat_idxs,d):\n",
        "        best_gain = -1\n",
        "        temp = 0\n",
        "        split_idx = None\n",
        "        split_threshold = None\n",
        "\n",
        "        for feat_idx in feat_idxs:\n",
        "            X_column = X[:, feat_idx]\n",
        "            X_col = X_column\n",
        "            thresholds = np.unique(X_column)\n",
        "\n",
        "            for thr in thresholds:\n",
        "                # calculate the information gain\n",
        "                gain = self.information_gain(y, X_column, thr,best_gain)\n",
        "                temp = gain\n",
        "\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    temp = gain\n",
        "                    split_idx = feat_idx\n",
        "                    split_threshold = thr\n",
        "\n",
        "        return split_idx, split_threshold\n",
        "\n",
        "\n",
        "    def information_gain(self, y, X_column, threshold,best_gain):\n",
        "        parent_entropy = _mce(y)\n",
        "\n",
        "        # create children\n",
        "        left_idxs = np.argwhere(X_column <= threshold).flatten()\n",
        "        right_idxs = np.argwhere(X_column > threshold).flatten()\n",
        "\n",
        "        if len(left_idxs) == 0 or len(right_idxs) == 0:\n",
        "            return 0\n",
        "        \n",
        "        n = len(y)\n",
        "        n_l, n_r = len(left_idxs), len(right_idxs)\n",
        "        e_l, e_r = _mce(y[left_idxs]), _mce(y[right_idxs])\n",
        "        child_entropy = (n_l/n) * e_l + (n_r/n) * e_r\n",
        "\n",
        "        # calculate the IG\n",
        "        information_gain = parent_entropy - child_entropy\n",
        "        return information_gain\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.array([self.traverse(x, self.root,0) for x in X])\n",
        "\n",
        "    def traverse(self, x, node,ht):\n",
        "        if node.is_leaf_node():\n",
        "            return node.value\n",
        "\n",
        "        if x[node.feature] <= node.threshold:\n",
        "            return self.traverse(x, node.left,ht)\n",
        "        else:\n",
        "            return self.traverse(x, node.right,ht)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee93dc80-27a8-466e-91f6-cc16d2f87187",
      "metadata": {
        "id": "ee93dc80-27a8-466e-91f6-cc16d2f87187"
      },
      "source": [
        "### `sklearn` Decision Tree Experiments\n",
        "\n",
        "1. Scikit-learn has two decision tree implementations: [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) and [`DecisionTreeRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html). \n",
        "\n",
        "When would you use one over the other? What would you use in the case of the banknote authentication dataset? Explain the changes that need to be made in the dataset to use the other implementation.\n",
        "\n",
        "2. Fit a decision tree to the training set. Change various parameters and compare them to one another. Mainly try and experiment with the `criterion`, `max_depth` and `min_samples_split` parameters. Report the accuracy on the training and test set for each of the experiments while varying the parameters for comparison purposes.\n",
        "\n",
        "3. Plot your trees !! (optional) (for visualization)\n",
        "\n",
        "```python\n",
        "from sklearn.tree import plot_tree\n",
        "\n",
        "def plotTree(tree):\n",
        "    \"\"\"\n",
        "    tree: Tree instance that is the result of fitting a DecisionTreeClassifier\n",
        "          or a DecisionTreeRegressor.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(30,20))\n",
        "    plot_tree(tree, filled=True, rounded=True,\n",
        "                  class_names=['forged', 'authentic'],\n",
        "                  feature_names=['var', 'skew', 'curt', 'ent'])\n",
        "    plt.show()\n",
        "    return None\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DecisionTreeClassifier is used for classification tasks, where the target variable is categorical. DecisionTreeRegressor is used for regression tasks, where the target variable is continuous.\n",
        "\n",
        "For the banknote authentication dataset, since the target variable is whether the note is authentic or not (categorical), you would use DecisionTreeClassifier.\n",
        "\n",
        "No changes need to be made to the dataset to use the other implementation, just change the classifier used (e.g. from DecisionTreeClassifier to DecisionTreeRegressor)."
      ],
      "metadata": {
        "id": "jtX4cUUsMa4G"
      },
      "id": "jtX4cUUsMa4G"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "2a4f8dc0-50a7-4634-87cf-d75ccf3b43c0",
      "metadata": {
        "id": "2a4f8dc0-50a7-4634-87cf-d75ccf3b43c0"
      },
      "outputs": [],
      "source": [
        "# your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af48e314-8c64-4fc2-b654-726d3549e42f",
      "metadata": {
        "id": "af48e314-8c64-4fc2-b654-726d3549e42f"
      },
      "source": [
        "### Load Data\n",
        "\n",
        "The data has been loaded onto a Pandas DataFrame. Try to get an initial feel for the data by using functions like `describe()`, `info()`, or maybe try to plot the data to check for any patterns.\n",
        "\n",
        "Note: To obtain the data from the UCI website, `wget` can be used followed by shuffling the samples using `shuf` and adding a header for easier reading via `pandas`. It is not necessary to view the data in a DataFrame and can be directly loaded onto NumPy as convenient."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ea8dd35b-09f6-4400-821b-d9aa0b07cd23",
      "metadata": {
        "id": "ea8dd35b-09f6-4400-821b-d9aa0b07cd23"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('bankAuth.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e083bc34-cf20-41f5-a1ff-889beb22e03a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e083bc34-cf20-41f5-a1ff-889beb22e03a",
        "outputId": "3400e542-b99c-4cf3-d235-5f49c1f52f4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Length:  1372\n",
            "Dataset Shape:  (1372, 5)\n",
            "Dataset: \n",
            "   variance  skewness  curtosis  entropy  target\n",
            "0   3.62160    8.6661   -2.8073 -0.44699       0\n",
            "1   4.54590    8.1674   -2.4586 -1.46210       0\n",
            "2   3.86600   -2.6383    1.9242  0.10645       0\n",
            "3   3.45660    9.5228   -4.0112 -3.59440       0\n",
            "4   0.32924   -4.4552    4.5718 -0.98880       0\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "def importdata():\n",
        "    balance_data = pd.read_csv('bankAuth.txt' , names=[\"variance\", \"skewness\", \"curtosis\" , \"entropy\",\"target\"])\n",
        "      \n",
        "    # Printing the dataswet shape\n",
        "    print (\"Dataset Length: \", len(balance_data))\n",
        "    print (\"Dataset Shape: \", balance_data.shape)\n",
        "      \n",
        "    # Printing the dataset obseravtions\n",
        "    print (\"Dataset: \")\n",
        "    print(balance_data.head())\n",
        "    return balance_data\n",
        "\n",
        "data = importdata()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da1cec0c-7433-4d38-875d-c9b25f2366b5",
      "metadata": {
        "id": "da1cec0c-7433-4d38-875d-c9b25f2366b5"
      },
      "source": [
        "### Splitting the Data\n",
        "\n",
        "It is a good practice to split the data into training and test sets. This is to ensure that the model is not overfitting to the training data. The test set is used to evaluate the performance of the model on unseen data. The test set is not used to train the model in any way. The test set is only used to evaluate the performance of the model. You may use the `train_test_split` function from `sklearn.model_selection` to split the data into training and test sets.\n",
        "\n",
        "It is a good idea to move your data to NumPy arrays now as it will make computing easier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ffc54272-3855-4601-8ad7-f3c3062a687a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffc54272-3855-4601-8ad7-f3c3062a687a",
        "outputId": "c121a274-47ce-4db9-f2f9-51051c119ddd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(960, 4)\n",
            "(960,)\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "# Function to split the dataset\n",
        "def splitdataset(balance_data):\n",
        "  \n",
        "    # Separating the target variable\n",
        "    X = balance_data.values[:, :-1]\n",
        "    Y = balance_data.values[:, 4]\n",
        "    # Splitting the dataset into train and test\n",
        "    X_train, X_test, y_train, y_test = train_test_split( \n",
        "    X, Y, test_size = 0.3, random_state = 100)\n",
        "      \n",
        "    return X, Y, X_train, X_test, y_train, y_test\n",
        "X, Y, X_train, X_test, y_train, y_test = splitdataset(data)\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63938d1f-174f-4054-ba7f-82d73171f2e4",
      "metadata": {
        "id": "63938d1f-174f-4054-ba7f-82d73171f2e4"
      },
      "source": [
        "### Denouement\n",
        "\n",
        "Use this place to report all comparisons and wrap up the calls to the functions written above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9acc1dad-a657-403b-9154-003961d241ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9acc1dad-a657-403b-9154-003961d241ac",
        "outputId": "8748761e-88a3-4747-f50f-41fed392d5e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best feature :  0\n",
            "Best Threshold 0.26877\n",
            "Best feature :  1\n",
            "Best Threshold 7.5032\n",
            "Best feature :  2\n",
            "Best Threshold -2.5237\n",
            "Best feature :  0\n",
            "Best Threshold -5.3857\n",
            "Best feature :  2\n",
            "Best Threshold -4.4738\n",
            "Best feature :  0\n",
            "Best Threshold 2.3917\n",
            "Best feature :  1\n",
            "Best Threshold -4.8835\n",
            "93.93203883495146\n"
          ]
        }
      ],
      "source": [
        "# your code here\n",
        "clf = DecisionTree(max_depth=3)\n",
        "clf.fit(X_train, y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "def accuracy(y_test, y_pred):\n",
        "    return np.sum(y_test == y_pred) / len(y_test)\n",
        "\n",
        "acc = accuracy(y_test, predictions)\n",
        "print(acc*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entropy : 95.63106796116504\n",
        "\n",
        "Gini : 94.1747572815534\n",
        "\n",
        "Misclassification Error : 93.93203883495146\n"
      ],
      "metadata": {
        "id": "xdQgz2vRzBEZ"
      },
      "id": "xdQgz2vRzBEZ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}